{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f929af0-e974-4d07-8f7d-95df5f44b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from sys import exit\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb2efdd5-d2c1-4327-8a20-bdcc9dc0407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define request headers\n",
    "REQ_HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\"\n",
    "}\n",
    "\n",
    "\n",
    "BASE_URL = \"https://arxiv.org\"\n",
    "\n",
    "# Define the base URL for scraping programming language from arxiv catchup from 1st Nov 2024\n",
    "SCRAPE_START_URL = \"https://www.arxiv.org/catchup/cs.PL/2024-11-01?abs=True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7925221-e3a4-4f8c-a2dc-14f5436b2aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(SCRAPE_START_URL, headers=REQ_HEADERS)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    page_content = response.content\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "    page_content = None\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b599d99-fd57-4d68-b15f-8ce4580ac744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<head>\n",
      " <title>\n",
      " </title>\n",
      " <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      " <link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n",
      " <link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n",
      " <link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n",
      " <link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n",
      " <link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n",
      " <meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n",
      " <meta content=\"#ffffff\" name=\"theme-color\"/>\n",
      " <link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\">\n",
      "  <link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\">\n",
      "   <link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\">\n",
      "    <script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\">\n",
      "    </script>\n",
      "    <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\">\n",
      "    </script>\n",
      "    <script language=\"javascript\" type=\"text/javascript\">\n",
      "     mathjaxToggle();\n",
      "    </script>\n",
      "   </link>\n",
      "  </link>\n",
      " </link>\n",
      "</head>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.head.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e9560c8-3d45-40ed-8b4d-c68dc1fa21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_meta_data = {}\n",
    "# Find the div containing the text \"Total of\"\n",
    "div_with_entries = soup.find(string=re.compile(r'Total of'))\n",
    "\n",
    "total_entries_on_this_page = 0\n",
    "current_page_date = \"\"\n",
    "current_page_formatted_date = \"\"\n",
    "# Extract the text from the relevant div\n",
    "if div_with_entries:\n",
    "    total_entries_text = div_with_entries.get_text()\n",
    "\n",
    "    # Use regular expressions to extract the total number of entries\n",
    "    entries_match = re.search(r'Total of (\\d+) entries', total_entries_text)\n",
    "    total_entries_on_this_page = int(entries_match.group(1)) if entries_match else 0\n",
    "\n",
    "    current_page_date = total_entries_text.split('entries for')[1].strip()\n",
    "\n",
    "    date_obj = datetime.strptime(current_page_date, '%a, %d %b %Y')\n",
    "    current_page_formatted_date = date_obj.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "next_page_url = None\n",
    "next_page_a_tag = soup.find('a', string='Continue to the next day')\n",
    "if next_page_a_tag:\n",
    "    next_page_url = BASE_URL + next_page_a_tag[\"href\"]\n",
    "\n",
    "\n",
    "page_meta_data = {\n",
    "    \"total_articles\": total_entries_on_this_page,\n",
    "    \"date_raw\": current_page_date,\n",
    "    \"date_clean\": current_page_formatted_date,\n",
    "    \"next_page_url\": next_page_url\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dbaaacc-6924-4116-946c-443abfec67ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_articles': 9,\n",
       " 'date_raw': 'Fri, 01 Nov 2024',\n",
       " 'date_clean': '2024-11-01',\n",
       " 'next_page_url': 'https://arxiv.org/catchup/cs.PL/2024-11-04?abs=True&page=1'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "251c4d72-befb-4039-b510-1abbcce84247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to hold the extracted data\n",
    "papers = []\n",
    "categories_meta_data = {\n",
    "    \"categories\": []\n",
    "}\n",
    "\n",
    "# Find all entries in the page\n",
    "articles = soup.find_all(\"dl\", id='articles')\n",
    "\n",
    "for entry in articles:\n",
    "    \n",
    "    dds = entry.find_all('dd') # meta\n",
    "    dts = entry.find_all('dt') # links\n",
    "\n",
    "    category = \"\"\n",
    "    category_tag = entry.find('h3')\n",
    "    if category_tag:\n",
    "        category = category_tag.text.split('(')[0].strip()\n",
    "        entries_match = re.search(r'of (\\d+) entries', category_tag.text)\n",
    "        total_articles_of_category = int(entries_match.group(1)) if entries_match else 0\n",
    "        categories_meta_data[\"categories\"].append(category)\n",
    "        categories_meta_data[category] = total_articles_of_category\n",
    "\n",
    "    for index, dd in enumerate(dds):\n",
    "        paper = {}\n",
    "        dt = dts[index]\n",
    "        paper['category'] = category\n",
    "        \n",
    "        # Extract the title\n",
    "        title_tag = dd.find('div', class_='list-title mathjax')\n",
    "        if title_tag:\n",
    "            paper['title'] = title_tag.text.replace('Title:', '').strip()\n",
    "        \n",
    "        # Extract the authors\n",
    "        authors_tag = dd.find('div', class_='list-authors')\n",
    "        if authors_tag:\n",
    "            paper['authors'] = authors_tag.text.replace('Authors:', '').strip()\n",
    "    \n",
    "        # Extract the comments\n",
    "        comments_tag = dd.find('div', class_='list-comments mathjax')\n",
    "        if comments_tag:\n",
    "            paper['comments'] = comments_tag.text.replace('Comments:', '').strip()\n",
    "    \n",
    "        # Extract the journal refrences\n",
    "        journal_ref_tag = dd.find('div', class_='list-journal-ref')\n",
    "        if journal_ref_tag:\n",
    "            paper['journal_refrence'] = journal_ref_tag.text.replace('Journal-ref:', '').strip()\n",
    "        \n",
    "        # Extract the description\n",
    "        description_tag = dd.find('p', class_='mathjax')\n",
    "        if description_tag:\n",
    "            paper['description'] = description_tag.text.strip()\n",
    "        \n",
    "        # Extract the subjects\n",
    "        subjects_tag = dd.find('div', class_='list-subjects')\n",
    "        if subjects_tag:\n",
    "            paper['subjects'] = subjects_tag.text.replace('Subjects:', '').strip()\n",
    "    \n",
    "        # Extract the abstract name and link\n",
    "        abs_tag = dt.find('a', title='Abstract')\n",
    "        if abs_tag:\n",
    "            paper['abs_name'] = abs_tag.text.strip()\n",
    "            paper['abs_link'] = BASE_URL + abs_tag['href']\n",
    "        \n",
    "        # Extract the PDF link\n",
    "        pdf_tag = dt.find('a', title='Download PDF')\n",
    "        if pdf_tag:\n",
    "            paper['pdf_link'] = BASE_URL + pdf_tag['href']\n",
    "        \n",
    "        # Add the paper to the list\n",
    "        papers.append(paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efb16130-929c-4c2c-8212-eb5481570e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'categories': ['New submissions',\n",
       "  'Cross submissions',\n",
       "  'Replacement submissions'],\n",
       " 'New submissions': 1,\n",
       " 'Cross submissions': 3,\n",
       " 'Replacement submissions': 5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67aaa15a-a620-4759-9514-da408fabeeb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7851889-cece-4ef7-a1f1-aa5d36700f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'New submissions',\n",
       " 'title': 'Abstract Continuation Semantics for Multiparty Interactions in Process Calculi based on CCS',\n",
       " 'authors': 'Eneia Nicolae Todoran (Dept. of Computer Science, Technical University of Cluj-Napoca), Gabriel Ciobanu (Academia Europaea)',\n",
       " 'comments': 'In Proceedings FROM 2024, arXiv:2410.23020',\n",
       " 'journal_refrence': 'EPTCS 410, 2024, pp. 18-37',\n",
       " 'description': 'We develop denotational and operational semantics designed with continuations for process calculi based on CCS extended with mechanisms offering support for multiparty interactions. We investigate the abstractness of this continuation semantics. We show that our continuation-based denotational models are weakly abstract with respect to the corresponding operational models.',\n",
       " 'subjects': 'Programming Languages (cs.PL); Distributed, Parallel, and Cluster Computing (cs.DC); Multiagent Systems (cs.MA)',\n",
       " 'abs_name': 'arXiv:2410.23761',\n",
       " 'abs_link': 'https://arxiv.org/abs/2410.23761',\n",
       " 'pdf_link': 'https://arxiv.org/pdf/2410.23761'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6b93048-ea67-412a-87fe-742480dcd7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_page_data = {\n",
    "    \"page_metadata\": page_meta_data,\n",
    "    \"page_categories_meta_data\": categories_meta_data,\n",
    "    \"page_articles\": papers\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e538e6f-a5f7-48ed-8022-fcc57b0312ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_metadata': {'total_articles': 9,\n",
       "  'date_raw': 'Fri, 01 Nov 2024',\n",
       "  'date_clean': '2024-11-01',\n",
       "  'next_page_url': 'https://arxiv.org/catchup/cs.PL/2024-11-04?abs=True&page=1'},\n",
       " 'page_categories_meta_data': {'categories': ['New submissions',\n",
       "   'Cross submissions',\n",
       "   'Replacement submissions'],\n",
       "  'New submissions': 1,\n",
       "  'Cross submissions': 3,\n",
       "  'Replacement submissions': 5},\n",
       " 'page_articles': [{'category': 'New submissions',\n",
       "   'title': 'Abstract Continuation Semantics for Multiparty Interactions in Process Calculi based on CCS',\n",
       "   'authors': 'Eneia Nicolae Todoran (Dept. of Computer Science, Technical University of Cluj-Napoca), Gabriel Ciobanu (Academia Europaea)',\n",
       "   'comments': 'In Proceedings FROM 2024, arXiv:2410.23020',\n",
       "   'journal_refrence': 'EPTCS 410, 2024, pp. 18-37',\n",
       "   'description': 'We develop denotational and operational semantics designed with continuations for process calculi based on CCS extended with mechanisms offering support for multiparty interactions. We investigate the abstractness of this continuation semantics. We show that our continuation-based denotational models are weakly abstract with respect to the corresponding operational models.',\n",
       "   'subjects': 'Programming Languages (cs.PL); Distributed, Parallel, and Cluster Computing (cs.DC); Multiagent Systems (cs.MA)',\n",
       "   'abs_name': 'arXiv:2410.23761',\n",
       "   'abs_link': 'https://arxiv.org/abs/2410.23761',\n",
       "   'pdf_link': 'https://arxiv.org/pdf/2410.23761'},\n",
       "  {'category': 'Cross submissions',\n",
       "   'title': 'Linear and non-linear relational analyses for Quantum Program Optimization',\n",
       "   'authors': 'Matthew Amy, Joseph Lunderville',\n",
       "   'comments': '30 pages, 13 figures, comments welcome!',\n",
       "   'journal_refrence': 'Proc. ACM Program. Lang. 9, POPL, Article 37 (January 2025), 33 pages',\n",
       "   'description': 'The phase folding optimization is a circuit optimization used in many quantum compilers as a fast and effective way of reducing the number of high-cost gates in a quantum circuit. However, existing formulations of the optimization rely on an exact, linear algebraic representation of the circuit, restricting the optimization to being performed on straightline quantum circuits or basic blocks in a larger quantum program.\\nWe show that the phase folding optimization can be re-cast as an \\\\emph{affine relation analysis}, which allows the direct application of classical techniques for affine relations to extend phase folding to quantum \\\\emph{programs} with arbitrarily complicated classical control flow including nested loops and procedure calls. Through the lens of relational analysis, we show that the optimization can be powered-up by substituting other classical relational domains, particularly ones for \\\\emph{non-linear} relations which are useful in analyzing circuits involving classical arithmetic. To increase the precision of our analysis and infer non-linear relations from gate sets involving only linear operations -- such as Clifford+$T$ -- we show that the \\\\emph{sum-over-paths} technique can be used to extract precise symbolic transition relations for straightline circuits. Our experiments show that our methods are able to generate and use non-trivial loop invariants for quantum program optimization, as well as achieve some optimizations of common circuits which were previously attainable only by hand.',\n",
       "   'subjects': 'Quantum Physics (quant-ph); Programming Languages (cs.PL)',\n",
       "   'abs_name': 'arXiv:2410.23493',\n",
       "   'abs_link': 'https://arxiv.org/abs/2410.23493',\n",
       "   'pdf_link': 'https://arxiv.org/pdf/2410.23493'},\n",
       "  {'category': 'Cross submissions',\n",
       "   'title': 'Leveraging Slither and Interval Analysis to build a Static Analysis Tool',\n",
       "   'authors': 'Stefan-Claudiu Susan (Alexandru Ioan Cuza University of Iasi, Department of Computer Science, Iasi, Romania)',\n",
       "   'comments': 'In Proceedings FROM 2024, arXiv:2410.23020',\n",
       "   'journal_refrence': 'EPTCS 410, 2024, pp. 150-166',\n",
       "   'description': 'Even though much progress has been made in identifying and mitigating smart contract vulnerabilities, we often hear about coding or design issues leading to great financial losses. This paper presents our progress toward finding defects that are sometimes not detected or completely detected by state-of-the-art analysis tools. Although it is still in its incipient phase, we developed a working solution built on top of Slither that uses interval analysis to evaluate the contract state during the execution of each instruction. To improve the accuracy of our results, we extend interval analysis by also considering the constraints imposed by specific instructions. We present the current solution architecture in detail and show how it could be extended to other static analysis techniques, including how it can be integrated with other third-party tools. Our current benchmarks contain examples of smart contracts that highlight the potential of this approach to detect certain code defects.',\n",
       "   'subjects': 'Logic in Computer Science (cs.LO); Programming Languages (cs.PL); Software Engineering (cs.SE)',\n",
       "   'abs_name': 'arXiv:2410.23766',\n",
       "   'abs_link': 'https://arxiv.org/abs/2410.23766',\n",
       "   'pdf_link': 'https://arxiv.org/pdf/2410.23766'},\n",
       "  {'category': 'Cross submissions',\n",
       "   'title': 'A Type System for Data Flow and Alias Analysis in ReScript',\n",
       "   'authors': 'Nicky Ask Lund (Department of Computer Science, Aalborg University), Hans Hüttel (Department of Computer Science, University of Copenhagen)',\n",
       "   'comments': 'In Proceedings FROM 2024, arXiv:2410.23020. A full version of this paper is available at arXiv:2408.11954',\n",
       "   'journal_refrence': 'EPTCS 410, 2024, pp. 116-132',\n",
       "   'description': 'ReScript is a strongly typed language that targets JavaScript, as an alternative to gradually typed languages, such as TypeScript. In this paper, we present a sound type system for data-flow analysis for a subset of the ReScript language, more specifically for a lambda-calculus with mutability and pattern matching. The type system is a local analysis that collects information about variables that are used at each program point as well as alias information.',\n",
       "   'subjects': 'Logic in Computer Science (cs.LO); Programming Languages (cs.PL)',\n",
       "   'abs_name': 'arXiv:2410.23984',\n",
       "   'abs_link': 'https://arxiv.org/abs/2410.23984',\n",
       "   'pdf_link': 'https://arxiv.org/pdf/2410.23984'},\n",
       "  {'category': 'Replacement submissions',\n",
       "   'title': 'Synthetic Programming Elicitation for Text-to-Code in Very Low-Resource Programming and Formal Languages',\n",
       "   'authors': 'Federico Mora, Justin Wong, Haley Lepe, Sahil Bhatia, Karim Elmaaroufi, George Varghese, Joseph E. Gonzalez, Elizabeth Polgreen, Sanjit A. Seshia',\n",
       "   'comments': '14 pages, 6 figures, 1 table',\n",
       "   'description': 'Recent advances in large language models (LLMs) for code applications have demonstrated remarkable zero-shot fluency and instruction following on challenging code related tasks ranging from test case generation to self-repair. Unsurprisingly, however, models struggle to compose syntactically valid programs in programming languages unrepresented in pre-training, referred to as very low-resource Programming Languages (VLPLs). VLPLs appear in crucial settings, including domain-specific languages for internal tools, tool-chains for legacy languages, and formal verification frameworks. Inspired by a technique called natural programming elicitation, we propose designing an intermediate language that LLMs \"naturally\" know how to use and which can be automatically compiled to a target VLPL. When LLMs generate code that lies outside of this intermediate language, we use compiler techniques to repair the code into programs in the intermediate language. Overall, we introduce \\\\emph{synthetic programming elicitation and compilation} (SPEAC), an approach that enables LLMs to generate syntactically valid code even for VLPLs. We empirically evaluate the performance of SPEAC in a case study for the UCLID5 formal verification language and find that, compared to existing retrieval and fine-tuning baselines, SPEAC produces syntactically correct programs more frequently and without sacrificing semantic correctness.',\n",
       "   'subjects': 'Programming Languages (cs.PL); Machine Learning (cs.LG)',\n",
       "   'abs_name': 'arXiv:2406.03636',\n",
       "   'abs_link': 'https://arxiv.org/abs/2406.03636',\n",
       "   'pdf_link': 'https://arxiv.org/pdf/2406.03636'},\n",
       "  {'category': 'Replacement submissions',\n",
       "   'title': 'Programming of Cellular Automata in C and C++',\n",
       "   'authors': 'Patrik Christen',\n",
       "   'comments': '5 pages, 1 figure',\n",
       "   'description': \"This study explores running times of different ways to program cellular automata in C and C++, i.e. looping through arrays by different means, the effect of structures and objects, and the choice of data structure (array versus vector in C++) and compiler (GNU gcc versus Apple clang). Using arrays instead of vectors, using pointers instead of array indices, using C instead of C++, and using structures and objects instead of primitive data types has little to negligible effects on the running time. Also, the choice of compiler has only a minor effect, except for simple update functions, in which case clang seems to find better ways to optimise it, especially for index-based access. If one is interested in multi-state cellular automata, structures and objects can be used without loss of performance in C and C++, respectively. The study supports the recommendation from practice to use vector in C++ and adds to it the use of index-based access for cellular automata. Future studies might investigate Apple's Metal shader or compiler optimisation, especially with respect to the update function.\",\n",
       "   'subjects': 'Programming Languages (cs.PL)',\n",
       "   'abs_name': 'arXiv:2410.10022',\n",
       "   'abs_link': 'https://arxiv.org/abs/2410.10022',\n",
       "   'pdf_link': 'https://arxiv.org/pdf/2410.10022'},\n",
       "  {'category': 'Replacement submissions',\n",
       "   'title': 'Target-Aware Implementation of Real Expressions',\n",
       "   'authors': 'Brett Saiki, Jackson Brough, Jonas Regehr, Jesús Ponce, Varun Pradeep, Aditya Akhileshwaran, Zachary Tatlock, Pavel Panchekha',\n",
       "   'description': \"New low-precision accelerators, vector instruction sets, and library functions make maximizing accuracy and performance of numerical code increasingly challenging. Two lines of work$\\\\unicode{x2013}$traditional compilers and numerical compilers$\\\\unicode{x2013}$attack this problem from opposite directions. Traditional compiler backends optimize for specific target environments but are limited in their ability to balance performance and accuracy. Numerical compilers trade off accuracy and performance, or even improve both, but ignore the target environment. We join aspects of both to produce Chassis, a target-aware numerical compiler.\\nChassis compiles mathematical expressions to operators from a target description, which lists the real expressions each operator approximates and estimates its cost and accuracy. Chassis then uses an iterative improvement loop to optimize for speed and accuracy. Specifically, a new instruction selection modulo equivalence algorithm efficiently searches for faster target-specific programs, while a new cost-opportunity heuristic supports iterative improvement. We demonstrate Chassis' capabilities on 9 different targets, including hardware ISAs, math libraries, and programming languages. Chassis finds better accuracy and performance trade-offs than both Clang (by 3.5x) or Herbie (by up to 2.0x) by leveraging low-precision accelerators, accuracy-optimized numerical helper functions, and library subcomponents.\",\n",
       "   'subjects': 'Programming Languages (cs.PL)',\n",
       "   'abs_name': 'arXiv:2410.14025',\n",
       "   'abs_link': 'https://arxiv.org/abs/2410.14025',\n",
       "   'pdf_link': 'https://arxiv.org/pdf/2410.14025'},\n",
       "  {'category': 'Replacement submissions',\n",
       "   'title': 'A Distribution Semantics for Probabilistic Term Rewriting',\n",
       "   'authors': 'Germán Vidal',\n",
       "   'comments': 'Submitted for publication',\n",
       "   'description': 'Probabilistic programming is becoming increasingly popular thanks to its ability to specify problems with a certain degree of uncertainty. In this work, we focus on term rewriting, a well-known computational formalism. In particular, we consider systems that combine traditional rewriting rules with probabilities. Then, we define a distribution semantics for such systems that can be used to model the probability of reducing a term to some value. We also show how to compute a set of \"explanations\" for a given reduction, which can be used to compute its probability. Finally, we illustrate our approach with several examples and outline a couple of extensions that may prove useful to improve the expressive power of probabilistic rewrite systems.',\n",
       "   'subjects': 'Programming Languages (cs.PL); Artificial Intelligence (cs.AI)',\n",
       "   'abs_name': 'arXiv:2410.15081',\n",
       "   'abs_link': 'https://arxiv.org/abs/2410.15081',\n",
       "   'pdf_link': 'https://arxiv.org/pdf/2410.15081'},\n",
       "  {'category': 'Replacement submissions',\n",
       "   'title': 'Coding Reliable LLM-based Integrated Task and Knowledge Agents with GenieWorksheets',\n",
       "   'authors': 'Harshit Joshi, Shicheng Liu, James Chen, Robert Weigle, Monica S. Lam',\n",
       "   'comments': 'preprint',\n",
       "   'description': 'Large Language Models (LLMs) present an opportunity to create automated assistants that can help users navigate complex tasks. However, existing approaches have limitations in handling conditional logic, integrating knowledge sources, and consistently following instructions. Researchers and industry professionals often employ ad hoc pipelines to construct conversational agents. These pipelines aim to maintain context, address failure cases, and minimize hallucinations, yet frequently fail to achieve these objectives. To this end, we present Genie - a programmable framework for creating task-oriented conversational agents that are designed to handle complex user interactions and knowledge queries. Unlike LLMs, Genie provides reliable grounded responses, with controllable agent policies through its expressive specification, Genie Worksheet. In contrast to dialog trees, it is resilient to diverse user queries, helpful with knowledge sources, and offers ease of programming policies through its declarative paradigm. The agents built using Genie outperforms the state-of-the-art method on complex logic domains in STARV2 dataset by up to 20.5%. Additionally, through a real-user study involving 62 participants, we show that Genie beats the GPT-4 with function calling baseline by 21.1%, 20.1%, and 61% on execution accuracy, dialogue act accuracy, and goal completion rate, respectively, on three diverse real-world domains',\n",
       "   'subjects': 'Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Programming Languages (cs.PL)',\n",
       "   'abs_name': 'arXiv:2407.05674',\n",
       "   'abs_link': 'https://arxiv.org/abs/2407.05674',\n",
       "   'pdf_link': 'https://arxiv.org/pdf/2407.05674'}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b413eaab-e0e0-4532-bd5c-f4e64b644b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'arxiv_programming_catchup_data_{current_page_formatted_date}.json', 'w') as f:\n",
    "    f.write(json.dumps(full_page_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54efda3-38f8-49c5-86d9-49381dd10a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
